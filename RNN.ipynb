{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_6_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d21xS_hNkvXI",
        "colab_type": "text"
      },
      "source": [
        "**Part 0:**\n",
        "\n",
        "Done: readings and scaffolding code\n",
        "\n",
        "\n",
        "**Part 1:**\n",
        "\n",
        "Done: Create RNN class that extends from nn module\n",
        "\n",
        "**Part 2:**\n",
        "\n",
        "Done:Fill in the pieces for sampling text.\n",
        "\n",
        "**Part 3:** \n",
        "\n",
        "Done: create an evaluation function/loop\n",
        "\n",
        "**Part 4:**\n",
        "\n",
        "To Do: Create a GRU Cell\n",
        "\n",
        "I ran out of time - I created a class with an init and forward function but got stuck on figuring out the dimensions for the matrix multiplication.\n",
        "\n",
        "**Part 5:**\n",
        "\n",
        "Done: Run it and generate some text (using built-in GRU)\n",
        "\n",
        "![alt text](https://nolans-cs-bucket.s3-us-west-1.amazonaws.com/ezgif.com-gif-maker(1).gif)\n",
        "\n",
        "**Part 6:**\n",
        "\n",
        "Done: Run it on a new dataset.  I trained on Emily Dickinson's poems and generated the following output. \n",
        "\n",
        "The network worked surprisingly well at picking up spelling, grammar, puncuation, and context. It however struggled in writing something longer than 200 characters, as it started repeating it's thoughts.\n",
        "\n",
        "**Output from Network:**\n",
        "'ng with despair. His head swam, but from the heat in his body he guessed that he had been given another draught.\n",
        "\n",
        "An Ore stooped over him, and flung him some bread and a strip of raw dried flesh. Hes head swam, but from the heat in his body he guessed that he had been given another draught.\n",
        "\n",
        "An Ore stooped over him, and flung him some bread and a strip of raw dried flesh. Hes head swam, but from the heat in his body he guessed that he had been given another draught.\n",
        "\n",
        "An Ore stooped over him, and flung him some bread and a strip of raw dried flesh. Hes head swam, but from the heat in his body he guessed that he had been given another draught.\n",
        "\n",
        "An Ore stooped over him, and flung him some bread and a strip of raw dried flesh. Hes head swam, but from the heat in his body he guessed that he had been given another draught.\n",
        "\n",
        "An Ore stooped over him, and flung him some bread and a strip of raw dried flesh. Hes head swam, but from the heat in his body he guessed that he had been give'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpOqH8rvcrbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Lab6.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1C7Kjvp4s0UiSrgbBhLYlhD8s5Iot59tv\n",
        "\"\"\"\n",
        "!pip3 install unidecode\n",
        "#import subprocess\n",
        "from PIL import Image, ImageDraw\n",
        "import urllib.request\n",
        "import os.path\n",
        "from os import path\n",
        "import tarfile\n",
        "from torch.nn.parameter import Parameter\n",
        "from PIL import Image, ImageDraw\n",
        "from PIL import ImageFont\n",
        "url='https://nolans-cs-bucket.s3-us-west-1.amazonaws.com/text.tar.gz'\n",
        "location = './text.tar.gz'\n",
        "if path.exists(location):\n",
        "    print('already downloaded!')\n",
        "else:\n",
        "    urllib.request.urlretrieve(url,location )\n",
        "\n",
        "\n",
        "data=tarfile.open(location)\n",
        "data.extractall()\n",
        "data.close()\n",
        "#subprocess.run(['pip','install','unidecode'])\n",
        "#! pip install unidecode\n",
        "#! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        " \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt',encoding='utf8').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  print(file[start_index:end_index])\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  #new_string=''    \n",
        "  #for i in range(len(string)):\n",
        "      #if string[i]=='\\n' and i<len(string) :\n",
        "          #new_string=new_string+' '\n",
        "      #else:\n",
        "          #new_string=new_string+string[i]\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return Variable(tensor)\n",
        "\n",
        "print(char_tensor('abcDEF'))\n",
        "\n",
        "##the encoder\n",
        "\n",
        "def make_strings(encoder,temperature):\n",
        "    epochs_eval=100\n",
        "    hidden_e = encoder.init_hidden()\n",
        "    \n",
        "    vecs=[]\n",
        "    for i in range(epochs_eval):\n",
        "        if i==0:\n",
        "            output,hidden=encoder(char_tensor('a'),hidden_e)\n",
        "        else:\n",
        "            output,hidden=encoder(output_char2,hidden)\n",
        "            output = output.squeeze(0).squeeze(0)\n",
        "            output = torch.exp(output)\n",
        "        #\n",
        "        #output_char=torch.argmax(output,dim=0)\n",
        "        output_char=torch.softmax(output/temperature,dim=0)\n",
        "        output_char2= torch.multinomial(output_char,1).squeeze(0).squeeze(0)\n",
        "        vecs.append(all_characters[output_char2.item()])\n",
        "    return(vecs)\n",
        "\n",
        "def evaluate_stuff(encoder,loss,epoch):\n",
        "    with torch.no_grad():\n",
        "       \n",
        "        vecs1=make_strings(encoder,2)\n",
        "        vecs2=make_strings(encoder,1) \n",
        "        vecs3=make_strings(encoder,0.5)\n",
        "        \n",
        "        \n",
        "        string=''\n",
        "        outstring1=string.join(vecs1)\n",
        "        string1=outstring1[0:50]\n",
        "        string2=outstring1[50:100]\n",
        "        \n",
        "        string=''\n",
        "        outstring2=string.join(vecs2)\n",
        "        string3=outstring2[0:50]\n",
        "        string4=outstring2[50:100]\n",
        "        \n",
        "        string=''\n",
        "        outstring3=string.join(vecs3)\n",
        "        string5=outstring3[0:50]\n",
        "        string6=outstring3[50:100]\n",
        "        \n",
        "         \n",
        "        img = Image.new('RGB', (1200, 800), color = (255, 255, 255))\n",
        "         \n",
        "  \n",
        "        fnt = ImageFont.truetype('arial.ttf', 40)\n",
        "        d = ImageDraw.Draw(img)\n",
        "        d.text((150, 50),'temperature : 2', font=fnt,fill=(0,0,0))\n",
        "        d.text((150, 100),string1, font=fnt,fill=(0,0,0))\n",
        "        d.text((150, 150),string2, font=fnt,fill=(0,0,0))\n",
        "        d.text((150, 250),'temperature : 1', font=fnt,fill=(0,0,0))\n",
        "        d.text((150, 300),string3, font=fnt,fill=(0,0,0))\n",
        "        d.text((150, 350),string4, font=fnt,fill=(0,0,0))\n",
        "        d.text((150, 450),'temperature : 0.5', font=fnt,fill=(0,0,0))\n",
        "        d.text((150, 500),string5, font=fnt,fill=(0,0,0))\n",
        "        d.text((150, 550),string6, font=fnt,fill=(0,0,0))\n",
        "        d.text((150, 700),\"epoch: \" + str(epoch), font=fnt,fill=(0,0,0))\n",
        "        d.text((700, 700),\"loss: \" + str(round(loss)), font=fnt,fill=(0,0,0))\n",
        "        #img.save('C:/Users/Nolan/Documents/image_' + str(epoch) + '.png')\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target\n",
        "##returns two vectors, each 200 in length.These are the indices of the letters in the \"all characters\" variable, a random chunk from the data\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    self.embedding = nn.Embedding(input_size,hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    self.net = nn.Sequential(nn.Linear(hidden_size,output_size))\n",
        "  def forward(self, input_char, hidden):\n",
        "    #pdb.set_trace()\n",
        "    embedded = self.embedding(input_char).view(1,1,-1)\n",
        "    output,hidden = self.gru(embedded,hidden)\n",
        "    #linear layer\n",
        "    linear_out=self.net(hidden)\n",
        "    #softmax - spit out probabilities\n",
        "    #output=torch.nn.functional.softmax(linear_out, dim=1)\n",
        "    return linear_out,hidden\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
        "  \n",
        "##the decoder\n",
        "    \n",
        "\n",
        "\n",
        "encoder=RNN(100,100,100)\n",
        "inp,target=random_training_set()\n",
        "learning_rate=0.001\n",
        "#mygru=myGRU(100,100)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "aws=0\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "  encoder = nn.DataParallel(encoder,device_ids=[0,1,2,3])\n",
        "  aws=1\n",
        "\n",
        "if torch.cuda.is_available()==1:\n",
        "  gpu=1\n",
        "  workers=8\n",
        "if torch.cuda.is_available()==0:\n",
        "  gpu=0\n",
        "  workers=2\n",
        "\n",
        "\n",
        "if gpu==1:\n",
        "\tif aws==1:\n",
        "\t\tencoder.to(device)\n",
        "\telse:\n",
        "\t\tencoder.cuda()\n",
        "else:\n",
        "  print('no cuda')\n",
        "#define the optimizer\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(),lr=1e-3)\n",
        "\n",
        "objective = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "def train(inp, target,encoder,encoder_optimizer,objective,gpu):\n",
        "  \n",
        "  encoder_optimizer.zero_grad()\n",
        "  input_length=inp.size(0)\n",
        "  encoder_outputs=torch.zeros(chunk_len,100,device=device,requires_grad=True)\n",
        "  \n",
        "  hidden_e = encoder.init_hidden()\n",
        "  hidden_e.require_grad=True\n",
        "  \n",
        "  #test=mygru(inp,hidden_e)\n",
        "  encoder_output=hidden_e\n",
        "  loss = 0.0\n",
        "  loss = torch.tensor(loss,requires_grad=True)\n",
        "  if gpu==1:\n",
        "      inp=inp.cuda()\n",
        "      encoder_output=encoder_output.cuda()\n",
        "      target = target.cuda()\n",
        "  \n",
        "  for i in range(input_length-1):\n",
        "    #pdb.set_trace()\n",
        "    if i==0:\n",
        "        encoder_hidden=hidden_e\n",
        "    encoder_output,encoder_hidden=encoder(inp[i],encoder_hidden)\n",
        "    encoder_outputs[i]=encoder_output[0,0]\n",
        "    x=encoder_output.squeeze(0)\n",
        "    test=inp[i+1].unsqueeze(0)\n",
        "    loss=torch.sum(loss+objective(x,test))\n",
        "  #decoder_input = torch.tensor([[0]],device=device)\n",
        "  \n",
        "  loss.backward()\n",
        "\n",
        "  encoder_optimizer.step()\n",
        " \n",
        "  return loss.item() \n",
        "  \n",
        " \n",
        "epochs=1000\n",
        "  # more stuff here...\n",
        "loss_v=[]\n",
        "for i in range(epochs):\n",
        "    output=train(inp,target,encoder,encoder_optimizer,objective,gpu)\n",
        "    loss_v.append(output)\n",
        "    #if loss_v[-1]<1:\n",
        "        #break\n",
        "    #if i%10==0:\n",
        "        #evaluate_stuff(encoder,output,i)\n",
        "    print(output)\n",
        "\n",
        "#with open('C:/Users/Nolan/Documents/RNN_model_2.txt', 'w') as f:\n",
        "#   out=torch.save(encoder.state_dict(),'C:/Users/Nolan/Documents/RNN_model_2.txt')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqgcXsWReV59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "encoder.eval()\n",
        "temperature=2\n",
        "epochs_eval=1000\n",
        "hidden_e = encoder.init_hidden()\n",
        "\n",
        "vecs=[]\n",
        "for i in range(epochs_eval):\n",
        "    if i==0:\n",
        "        output,hidden=encoder(char_tensor('a'),hidden_e)\n",
        "    else:\n",
        "        output,hidden=encoder(output_char2,hidden)\n",
        "    output = output.squeeze(0).squeeze(0)\n",
        "    output = torch.exp(output)\n",
        "    #\n",
        "    #output_char=torch.argmax(output,dim=0)\n",
        "    output_char=torch.softmax(output/temperature,dim=0)\n",
        "    output_char2= torch.multinomial(output_char,1).squeeze(0).squeeze(0)\n",
        "    vecs.append(all_characters[output_char2.item()])\n",
        "\n",
        "string=''\n",
        "outstring=string.join(vecs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}